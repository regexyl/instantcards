from typing import Dict, Any, List, NamedTuple, Optional
import structlog
import MeCab
from .classes import Block, Translation, Atom

logger = structlog.get_logger()


class MeCabToken(NamedTuple):
    """Structured representation of a MeCab token."""
    surface: str  # è¡¨å±¤å½¢ (surface form)
    pos: str  # å“è© (part of speech)
    pos_details: Optional[list[str]]  # å“è©ç´°åˆ†é¡
    conjugation_type: Optional[str]  # æ´»ç”¨å‹
    conjugation_form: Optional[str]  # æ´»ç”¨å½¢
    conjugation_form_number: Optional[str]  # æ´»ç”¨å½¢ç•ªå·
    base_form: str  # åŸå½¢ (base form)
    reading: str  # èª­ã¿ (reading/pronunciation)
    pronunciation: str  # ç™ºéŸ³ (pronunciation)

    def get_metadata(self) -> dict:
        return {
            "pos_details": self.pos_details,
            "conjugation_type": self.conjugation_type,
            "conjugation_form": self.conjugation_form,
            "reading": self.reading,
            "pronunciation": self.pronunciation
        }


def extract_atoms(translation: Translation, job_id: str) -> Dict[str, Any]:
    """
    Extract vocabulary words from transcription using Japanese tokenizer and create Atom objects.
    Processes each block individually to maintain context.

    Args:
        translation: Translation object containing transcription data
        job_id: Unique job identifier

    Returns:
        Dictionary with atom extraction results
    """
    logger.info("extracting_atoms", job_id=job_id,
                block_count=translation.get_block_count())

    try:
        total_atoms = 0
        blocks_with_atoms = 0
        unique_atoms = set()

        # Process each block individually
        for block_index, block in enumerate(translation.blocks):
            block_atoms = _extract_atoms_from_block(block, block_index, job_id)

            if block_atoms:
                translation.add_atoms_to_block(block_index, block_atoms)
                total_atoms += len(block_atoms)
                blocks_with_atoms += 1
                for atom in block_atoms:
                    unique_atoms.add(atom.value)

        result_data = {
            "atoms_extracted": total_atoms,
            "unique_atoms_count": len(unique_atoms),
            "blocks_with_atoms": blocks_with_atoms,
            "total_blocks_processed": len(translation.blocks),
            "average_atoms_per_block": total_atoms / len(translation.blocks) if translation.blocks else 0
        }

        logger.info("atoms_extracted",
                    job_id=job_id,
                    atoms_count=total_atoms,
                    unique_atoms=len(unique_atoms),
                    blocks_with_atoms=blocks_with_atoms)

        return result_data

    except Exception as e:
        logger.exception("atoms_extraction_failed",
                         job_id=job_id, error=str(e))
        raise


def _extract_atoms_from_block(block: Block, block_index: int, job_id: str) -> List[Atom]:
    """
    Extract atoms from a single block of text.

    Args:
        block: Block object containing text to analyze
        block_index: Index of the block for logging
        job_id: Job identifier for logging

    Returns:
        List of Atom objects extracted from this block
    """
    if not block.value.strip():
        return []

    logger.debug("processing_block", job_id=job_id, block_index=block_index,
                 text_length=len(block.value))

    tokens = _parse_mecab_output(block.value)

    atoms = []
    seen_words = set()

    for token in tokens:
        word = token.surface
        base_form = token.base_form if token.base_form != '*' else word

        if word not in seen_words:
            atom = Atom(value=word, base_form=base_form,
                        part_of_speech=token.pos, metadata=token.get_metadata())
            atoms.append(atom)
            seen_words.add(word)

    logger.debug("block_atoms_extracted", job_id=job_id, block_index=block_index,
                 atoms_count=len(atoms))

    return atoms


def _parse_mecab_output(text: str) -> List[MeCabToken]:
    """
    Parse MeCab output into structured token objects.

    Args:
        text: Japanese text to analyze

    Returns:
        List of MeCabToken objects with parsed information
    """
    tagger = MeCab.Tagger()
    parsed = tagger.parse(text)

    tokens = []

    for line in parsed.split('\n'):
        if line == 'EOS' or not line.strip():
            continue

        # Parse MeCab output format:
        # è¡¨å±¤å½¢\tèª­ã¿\tç™ºéŸ³\tåŸå½¢\tå“è©-å“è©ç´°åˆ†é¡1\tæ´»ç”¨å‹\tæ´»ç”¨å½¢\tæ´»ç”¨å½¢è©³ç´°\tæ´»ç”¨å½¢ç•ªå·
        # surface_form\treading\tpronunciation\tbase_form\tpart_of_speech-part_of_speech_detail1\tconjugation_type\tconjugation_form\tconjugation_form_detail\tconjugation_form_number
        parts = line.split('\t')
        # print(f"ğŸ˜¸ {parts}")
        if len(parts) < 2:
            continue

        surface, reading, pronunciation, base_form, pos_info, conjugation_type, conjugation_form, conjugation_form_number = parts + \
            [None] * (8 - len(parts))
        pos_info = pos_info.split('-')
        pos = pos_info[0]
        pos_rest_of_details = pos_info[1:]
        pos_en = _map_part_of_speech(pos, surface)

        if pos_en in ['symbol', 'auxiliary_symbol']:
            continue

        token = MeCabToken(
            surface=surface,
            pos=pos_en,
            pos_details=pos_rest_of_details,
            conjugation_type=conjugation_type,
            conjugation_form=conjugation_form,
            conjugation_form_number=conjugation_form_number,
            base_form=base_form,
            reading=reading,
            pronunciation=pronunciation
        )

        tokens.append(token)

    return tokens


def _map_part_of_speech(mecab_pos: str, word: str) -> str:
    """Map MeCab part-of-speech to English equivalents."""
    pos_mapping = {
        'åè©': 'noun',
        'å‹•è©': 'verb',
        'å½¢å®¹è©': 'adjective',
        'å½¢å®¹å‹•è©': 'adjectival_verb',
        'å‰¯è©': 'adverb',
        'åŠ©è©': 'particle',
        'åŠ©å‹•è©': 'auxiliary_verb',
        'æ¥ç¶šè©': 'conjunction',
        'ä»£åè©': 'pronoun',
        'é€£ä½“è©': 'determiner',
        'æ„Ÿå‹•è©': 'interjection',
        'è¨˜å·': 'symbol',
        'æ¥é ­è©': 'prefix',
        "æ¥é ­è¾": "prefix",
        'æ¥å°¾è¾': 'suffix',
        "è£œåŠ©è¨˜å·": "auxiliary_symbol",
        'ãƒ•ã‚£ãƒ©ãƒ¼': 'filler',
        'ãã®ä»–': 'other',
        'æœªçŸ¥èª': 'unknown',
    }

    mapped_pos = pos_mapping.get(mecab_pos, 'other')

    if mapped_pos == 'other' and mecab_pos not in pos_mapping:
        logger.error("part_of_speech_mapping_failed",
                     word=word,
                     mecab_pos=mecab_pos,
                     mapped_to=mapped_pos)

    return mapped_pos
